---
title: "Multinomial Logit Model"
author: "Haoxuan Li"
date: today
---



This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{r}
# set seed for reproducibility
set.seed(123)

# define attributes
brand <- c("N", "P", "H") # Netflix, Prime, Hulu
ad <- c("Yes", "No")
price <- seq(8, 32, by=4)

# generate all possible profiles
profiles <- expand.grid(
    brand = brand,
    ad = ad,
    price = price
)
m <- nrow(profiles)

# assign part-worth utilities (true parameters)
b_util <- c(N = 1.0, P = 0.5, H = 0)
a_util <- c(Yes = -0.8, No = 0.0)
p_util <- function(p) -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps <- 100
n_tasks <- 10
n_alts <- 3

# function to simulate one respondentâ€™s data
sim_one <- function(id) {
  
    datlist <- list()
    
    # loop over choice tasks
    for (t in 1:n_tasks) {
        
        # randomly sample 3 alts (better practice would be to use a design)
        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])
        
        # compute deterministic portion of utility
        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat$e <- -log(-log(runif(n_alts)))
        dat$u <- dat$v + dat$e
        
        # identify chosen alternative
        dat$choice <- as.integer(dat$u == max(dat$u))
        
        # store task
        datlist[[t]] <- dat
    }
    
    # combine all tasks for one respondent
    do.call(rbind, datlist)
}

# simulate data for all respondents
conjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))

# remove values unobservable to the researcher
conjoint_data <- conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]

# clean up
rm(list=setdiff(ls(), "conjoint_data"))
```
::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.


```{r}
str(conjoint_data)
head(conjoint_data)
table(conjoint_data$choice)

```
```{r}
# Convert brand into dummy variables: using H as the baseline
conjoint_data$brand_N <- as.integer(conjoint_data$brand == "N")
conjoint_data$brand_P <- as.integer(conjoint_data$brand == "P")
# H is implicitly the base level

# Convert ad into dummy: 1 = Yes (ads shown), 0 = No (ad-free)
conjoint_data$ad_yes <- as.integer(conjoint_data$ad == "Yes")

# Final modeling matrix X
X <- as.matrix(conjoint_data[, c("brand_N", "brand_P", "ad_yes", "price")])

# Dependent variable
y <- conjoint_data$choice

# Preview prepared data
head(cbind(X, choice = y))

```

## 4. Estimation via Maximum Likelihood


```{r}
log_likelihood <- function(beta, X, y) {
  Xbeta <- X %*% beta
  expXbeta <- exp(Xbeta)

  group <- rep(1:(nrow(X)/3), each = 3)
  denom <- rep(tapply(expXbeta, group, sum), each = 3)

  p <- expXbeta / denom
  ll <- sum(y * log(p + 1e-15)) 
  return(-ll) 
}

init_beta <- rep(0, 4)
result <- optim(init_beta, log_likelihood, X = X, y = y, hessian = TRUE)

beta_hat <- result$par
se <- sqrt(diag(solve(result$hessian)))
ci <- cbind(beta_hat - 1.96 * se, beta_hat + 1.96 * se)
colnames(ci) <- c("Lower 95%", "Upper 95%")

beta_hat
ci
```


## 5. Estimation via Bayesian Methods


```{r}
log_posterior <- function(beta, X, y) {
  # log-likelihood (reuse your earlier function)
  Xbeta <- X %*% beta
  expXbeta <- exp(Xbeta)

  group <- rep(1:(nrow(X)/3), each = 3)
  denom <- rep(tapply(expXbeta, group, sum), each = 3)
  log_lik <- sum(y * log(expXbeta / denom + 1e-12))  # avoid log(0)

  # log-prior: 3 ~ N(0, 5^2), 1 ~ N(0, 1^2)
  log_prior <- sum(dnorm(beta[1:3], mean = 0, sd = 5, log = TRUE)) +
               dnorm(beta[4], mean = 0, sd = 1, log = TRUE)

  return(log_lik + log_prior)
}
```

set.seed(123)
```{r}
set.seed(123)
n_iter <- 11000
burn_in <- 1000
n_keep <- n_iter - burn_in
n_params <- 4

beta_curr <- rep(0, n_params)
post_curr <- log_posterior(beta_curr, X, y)


samples <- matrix(NA, nrow = n_keep, ncol = n_params)

proposal_sd <- c(0.05, 0.05, 0.05, 0.005)

accept <- 0
for (i in 1:n_iter) {
  beta_prop <- rnorm(n_params, mean = beta_curr, sd = proposal_sd)
  post_prop <- log_posterior(beta_prop, X, y)

  log_accept_ratio <- post_prop - post_curr
  if (log(runif(1)) < log_accept_ratio) {
    beta_curr <- beta_prop
    post_curr <- post_prop
    if (i > burn_in) accept <- accept + 1
  }

  if (i > burn_in) {
    samples[i - burn_in, ] <- beta_curr
  }
}
accept_rate <- accept / n_keep
```
```{r}
# Posterior summary
posterior_mean <- colMeans(samples)
posterior_sd <- apply(samples, 2, sd)
posterior_ci <- apply(samples, 2, quantile, probs = c(0.025, 0.975))

posterior_mean
posterior_sd
posterior_ci
```

```{r}
param_index <- 4
param_name <- "beta_price"

# Trace plot
plot(samples[, param_index], type = "l", col = "steelblue", 
     main = paste("Trace Plot of", param_name),
     xlab = "Iteration", ylab = "Value")

# Histogram
hist(samples[, param_index], breaks = 40, col = "skyblue", border = "white",
     main = paste("Posterior of", param_name),
     xlab = "Value")

```


## 6. Discussion

Suppose we did not simulate the data and treated it as real consumer behavior. Based on the estimation results:

- $\beta_{\text{Netflix}} > \beta_{\text{Prime}}$ implies that, holding other factors constant, consumers prefer Netflix over Prime, and Prime over Hulu (which is the reference brand). This ordering is consistent with the preference structure we simulated, and is also realistic in many real-world streaming contexts.

- $\beta_{\text{price}}$ is negative, which makes intuitive economic sense: as price increases, utility decreases, and thus the probability of choosing that option drops. This is a common and expected pattern in choice modeling.

---

In order to simulate and estimate a **multi-level (aka hierarchical)** model:

Instead of assuming fixed coefficients $\beta$ across all respondents, we allow each individual $i$ to have their own $\beta_i$, drawn from a population distribution:

$$
\beta_i \sim \mathcal{N}(\mu, \Sigma)
$$

When simulating data, we draw a different $\beta_i$ for each person and generate their choices based on that.

For estimation, we need to infer both the population-level parameters $(\mu, \Sigma)$ and the individual-level $\beta_i$'s.  
This typically requires **Bayesian methods** (e.g., Gibbs sampling or HMC), or **maximum simulated likelihood (MSL)** with tools like `bayesm` or `rstan`.

Such hierarchical models better capture **real-world preference heterogeneity**, and are especially useful for **individual-level targeting**, segmentation, and improving predictive accuracy.












